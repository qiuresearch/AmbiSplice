project: AmbiSplice
debug: false
model:
  dropout: 0.1
  loss: se3_vf_loss
  loss_scale_cls: 0.5
  loss_scale_psi: 0.5
dataloader:
  batch_size: 64
  train_batch_size: 64
  val_batch_size: 128
  test_batch_size: 128
  predict_batch_size: 512
  num_workers: 8
  prefetch_factor: 4
dataset:
  train_size: 32000
  val_size: 6400
  stratified_sampling: chrom
  dynamic_weights: false
  weighted_sampling: null
litrun:
  seed: 123
  num_devices: 1
  gpu_num: 7
  warm_start: null
  warm_start_cfg_override: false
  wandb:
    name: model
    project: ${project}
    save_code: false
    tags: []
    mode: online
  optimizer:
    learning_rate: 0.0007
    eps: 1e-8
    lr_scheduler: CosineAnnealingWarmRestarts
    lr_interval: epoch
    lr_T_0: 3
    lr_T_mult: 2
    last_lr_step: -1
  trainer:
    overfit_batches: 0
    min_epochs: 2
    max_epochs: 100
    max_time: "5:20:13:14"  # D:HH:MM:SS
    accelerator: gpu
    log_every_n_steps: 1
    deterministic: false
    strategy: ddp
    check_val_every_n_epoch: 1
    accumulate_grad_batches: 1
  callbacks:
    early_stopping:
      monitor: val/loss
      mode: min
      patience: 23
      strict: true
      verbose: true
    lr_monitor:
      logging_interval: step
      log_momentum: false
      log_lr: true
  checkpoints:
    dirpath: checkpoints/
    filename: '{epoch:03d}-{step:06d}'
    save_last: true
    save_top_k: 15
    monitor: train/loss
    mode: min
    every_n_epochs: 1
    every_n_train_steps: null